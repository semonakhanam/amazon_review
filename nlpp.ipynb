{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Stuning even for the non-gamer: This sound tr...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>The best soundtrack ever to anything.: I'm re...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Amazing!: This soundtrack is my favorite musi...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Excellent Soundtrack: I truly like this sound...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Remember, Pull Your Jaw Off The Floor After H...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>A Book That Is Worth a Second Look: This book...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>Best game ever: This games makes even amazing...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>Guitar in Absentia: With all due respect to a...</td>\n",
       "      <td>__label__1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>Stiff and Smells like drying paint: You get w...</td>\n",
       "      <td>__label__1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>Review of Pillow: This was a joke. I am sendi...</td>\n",
       "      <td>__label__1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>199 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Review        Label\n",
       "0     Stuning even for the non-gamer: This sound tr...  __label__2 \n",
       "1     The best soundtrack ever to anything.: I'm re...  __label__2 \n",
       "2     Amazing!: This soundtrack is my favorite musi...  __label__2 \n",
       "3     Excellent Soundtrack: I truly like this sound...  __label__2 \n",
       "4     Remember, Pull Your Jaw Off The Floor After H...  __label__2 \n",
       "..                                                 ...          ...\n",
       "194   A Book That Is Worth a Second Look: This book...  __label__2 \n",
       "195   Best game ever: This games makes even amazing...  __label__2 \n",
       "196   Guitar in Absentia: With all due respect to a...  __label__1 \n",
       "197   Stiff and Smells like drying paint: You get w...  __label__1 \n",
       "198   Review of Pillow: This was a joke. I am sendi...  __label__1 \n",
       "\n",
       "[199 rows x 2 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "nlp_ds=pd.read_csv('Amazon_Reviews.csv')\n",
    "nlp_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Stuning even for the non-gamer: This sound tr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>The best soundtrack ever to anything.: I'm re...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Amazing!: This soundtrack is my favorite musi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Excellent Soundtrack: I truly like this sound...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Remember, Pull Your Jaw Off The Floor After H...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>A Book That Is Worth a Second Look: This book...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>Best game ever: This games makes even amazing...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>Guitar in Absentia: With all due respect to a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>Stiff and Smells like drying paint: You get w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>Review of Pillow: This was a joke. I am sendi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>199 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Review  Label\n",
       "0     Stuning even for the non-gamer: This sound tr...      1\n",
       "1     The best soundtrack ever to anything.: I'm re...      1\n",
       "2     Amazing!: This soundtrack is my favorite musi...      1\n",
       "3     Excellent Soundtrack: I truly like this sound...      1\n",
       "4     Remember, Pull Your Jaw Off The Floor After H...      1\n",
       "..                                                 ...    ...\n",
       "194   A Book That Is Worth a Second Look: This book...      1\n",
       "195   Best game ever: This games makes even amazing...      1\n",
       "196   Guitar in Absentia: With all due respect to a...      0\n",
       "197   Stiff and Smells like drying paint: You get w...      0\n",
       "198   Review of Pillow: This was a joke. I am sendi...      0\n",
       "\n",
       "[199 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "nlp_ds['Label'].replace('__label__2 ',1,inplace=True)\n",
    "nlp_ds['Label'].replace('__label__1 ',0,inplace=True)\n",
    "        \n",
    "nlp_ds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=nlp_ds['Label']\n",
    "nlp_ds.drop(columns='Label',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(nlp_ds,y,test_size=0.2,random_state=43) \n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56     label memphi tn come reatard front cours wild ...\n",
       "37     great play larri work muse label late 80 earli...\n",
       "67     even mommi fun one four year old daughter love...\n",
       "79     creepi wow peopl behind jealousi reveng door o...\n",
       "80     audio option non exist movi better expect didn...\n",
       "157    rest stori knew someth miss first book read ma...\n",
       "177    make fascin read whale naturalist anim commun ...\n",
       "10     worst complet wast time typograph error poor g...\n",
       "128    technic inform book great view joint use build...\n",
       "62     scrape across whole top purchas screen last we...\n",
       "65     profound truli horribl narr style work famou f...\n",
       "17     fashion compress stock dvt doctor requir wear ...\n",
       "133    good manual expect book full glossi sharp pict...\n",
       "194    book worth second look book great inform easi ...\n",
       "146    bad product possibl broadway theatr archiv pro...\n",
       "38     work mac clearli say line work mac os system d...\n",
       "172    wonder worth 80 great block definit worth list...\n",
       "149    good song bad cover cd compil set purchas love...\n",
       "93     slow dull classic 1980 style movi 2 star 5 slo...\n",
       "29     base review bought one glad vcr dvd earli chri...\n",
       "0      stune even non gamer sound track beauti paint ...\n",
       "2      amaz soundtrack favorit music time hand intens...\n",
       "122    good gave 5 star littl dog much fun ripe apart...\n",
       "179    yet anoth unsubstanti case believ discrimin ce...\n",
       "95     worth time book wriiten horribl would never li...\n",
       "121    letter home like letter home cuz tell stori re...\n",
       "184    ok watch watch last night rememb enough give i...\n",
       "39     frustrat three year old son excit get two atte...\n",
       "66     definit enjoy gift 6 yr old daughter big barbi...\n",
       "19     size recomend size chart real size much smalle...\n",
       "11     great book great book could put could read fas...\n",
       "45     fast mp3 download music wax decad ago one 26 s...\n",
       "41     cannot recommend former alaskan want repeatedl...\n",
       "92     omg soulwax own wow like amaz album ever everi...\n",
       "167    cute gift littl babi although felt look bit ch...\n",
       "1      best soundtrack ever anyth read lot review say...\n",
       "57     either 1 5 star depend look either 1 5 star de...\n",
       "188    worst american b movi movi horror film thhat d...\n",
       "151    fool bought set last novemb decid listen cd no...\n",
       "166    would expect better bought two reason block ed...\n",
       "Name: Cleaned_text, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenize import tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tokenizer= RegexpTokenizer(r'\\w+')\n",
    "lemmatizer= WordNetLemmatizer()\n",
    "stemmer= PorterStemmer()\n",
    "\n",
    "\n",
    "\n",
    "def preprocessing(reviews):\n",
    "    final_tokens=' '\n",
    "    tokens=tokenizer.tokenize(reviews)\n",
    "         #print('Tokens:',tokens)\n",
    "    pure_tokens=[token.lower() for token in tokens if token.lower() not in stopwords.words('english')]\n",
    "    stemmed_tokens=[stemmer.stem(pure_token) for pure_token in pure_tokens]\n",
    "    final_tokens=final_tokens.join(stemmed_tokens)\n",
    "    return final_tokens\n",
    "     \n",
    "preprocessing('I was eating breakfast when you were playing')\n",
    "x_train['Cleaned_text']=x_train['Review'].apply(preprocessing)\n",
    "x_train['Cleaned_text']\n",
    "\n",
    "x_test['Cleaned_text']=x_test['Review'].apply(preprocessing)\n",
    "x_test['Cleaned_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'countri'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('countries',pos='n')\n",
    "stemmer.stem('countries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>Amazing: i have ordered this cd of here but i...</td>\n",
       "      <td>amaz order cd take abit get live england danc ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>Henry has come home...: I had Henry back in t...</td>\n",
       "      <td>henri come home henri back earli 70 forti stil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>80's High Tech with Rock Emotion: Although al...</td>\n",
       "      <td>80 high tech rock emot although mr hine work h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>Dirty South: This is probably the best souths...</td>\n",
       "      <td>dirti south probabl best southsid cd ever come...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>Perfect for the die-hard baseball fan: I have...</td>\n",
       "      <td>perfect die hard basebal fan read basebal amer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>Hunting the Hard Way: Thia was a gift for my ...</td>\n",
       "      <td>hunt hard way thia gift husband love book arri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>Delicious cookie mix: I thought it was funny ...</td>\n",
       "      <td>delici cooki mix thought funni bought product ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>*** BEWARE ***: This TV is set so that it is ...</td>\n",
       "      <td>bewar tv set capabl recal function want flash ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>Didn't live up to expectations.: I ordered th...</td>\n",
       "      <td>live expect order shoe replac previou pair rip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>Great combination of creativity and adventure...</td>\n",
       "      <td>great combin creativ adventur 4 year old daugh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Review  \\\n",
       "116   Amazing: i have ordered this cd of here but i...   \n",
       "138   Henry has come home...: I had Henry back in t...   \n",
       "155   80's High Tech with Rock Emotion: Although al...   \n",
       "82    Dirty South: This is probably the best souths...   \n",
       "159   Perfect for the die-hard baseball fan: I have...   \n",
       "..                                                 ...   \n",
       "58    Hunting the Hard Way: Thia was a gift for my ...   \n",
       "21    Delicious cookie mix: I thought it was funny ...   \n",
       "49    *** BEWARE ***: This TV is set so that it is ...   \n",
       "64    Didn't live up to expectations.: I ordered th...   \n",
       "68    Great combination of creativity and adventure...   \n",
       "\n",
       "                                          Cleaned_text  \n",
       "116  amaz order cd take abit get live england danc ...  \n",
       "138  henri come home henri back earli 70 forti stil...  \n",
       "155  80 high tech rock emot although mr hine work h...  \n",
       "82   dirti south probabl best southsid cd ever come...  \n",
       "159  perfect die hard basebal fan read basebal amer...  \n",
       "..                                                 ...  \n",
       "58   hunt hard way thia gift husband love book arri...  \n",
       "21   delici cooki mix thought funni bought product ...  \n",
       "49   bewar tv set capabl recal function want flash ...  \n",
       "64   live expect order shoe replac previou pair rip...  \n",
       "68   great combin creativ adventur 4 year old daugh...  \n",
       "\n",
       "[159 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer=TfidfVectorizer(stop_words='english',use_idf=True)\n",
    "\n",
    "vectorizer.fit(x_train['Cleaned_text'])\n",
    "x_train_Tfidf=vectorizer.transform(x_train['Cleaned_text'])\n",
    "x_test_Tfidf=vectorizer.transform(x_test['Cleaned_text'])\n",
    "\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,roc_auc_score,roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "clf= MultinomialNB().fit(x_train_Tfidf.toarray(),y_train)\n",
    "# clf.score(x_train_Tfidf.toarray(),y_train)\n",
    "    \n",
    "# test_review='that product was its absolute best and i loved using every feature of it'\n",
    "\n",
    "# x_test['Cleaned_text']=x_\n",
    "\n",
    "y_pred=clf.predict(x_test_Tfidf.toarray())\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5, 14],\n",
       "       [ 0, 21]], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.67483925, 0.72325161, 0.84644915, 0.61242925, 0.57193165,\n",
       "       0.59347948, 0.6355005 , 0.28671832, 0.71410611, 0.46206448,\n",
       "       0.57931907, 0.50660554, 0.67728853, 0.73264872, 0.55378304,\n",
       "       0.49391372, 0.73861835, 0.63477091, 0.58646245, 0.52199932,\n",
       "       0.62318359, 0.70846614, 0.63929356, 0.51773895, 0.62460531,\n",
       "       0.79876369, 0.60532193, 0.63863932, 0.70671061, 0.5231362 ,\n",
       "       0.70365441, 0.69767511, 0.67063017, 0.76582366, 0.62143117,\n",
       "       0.6418132 , 0.43659472, 0.65966975, 0.37741969, 0.73768669])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_proba_pred=clf.predict_proba(x_test_Tfidf.toarray())[::,1]\n",
    "y_proba_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import classification_report\n",
    "Accuracy_Score = accuracy_score(y_test,y_pred)\n",
    "Accuracy_Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg=LogisticRegression()\n",
    "logreg.fit(x_train_Tfidf,y_train)\n",
    "y_pred=logreg.predict(x_test_Tfidf)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7368421052631579"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparisons_df=pd.DataFrame({'Actuals':y_test,'predictions':y_pred})\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,precision_score,recall_score,f1_score\n",
    "\n",
    "confusion_matrix(y_test,y_pred)\n",
    "\n",
    "recall_score(y_test,y_pred)\n",
    "precision_score(y_test,y_pred)\n",
    "\n",
    "f1_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV\n",
    "decreg=DecisionTreeClassifier()\n",
    "\n",
    "new_dict={'max_depth':[2,3,4],\n",
    "          'max_leaf_nodes':[2,3,6],\n",
    "          'min_samples_leaf':[5,4,3,2],\n",
    "          'min_samples_split':[5,3,4]}\n",
    "\n",
    "gcv=GridSearchCV(decreg,param_grid=new_dict,cv=3)\n",
    "gcv.fit(x_train_Tfidf,y_train)\n",
    "\n",
    "decreg_pred=gcv.predict(x_test_Tfidf)\n",
    "decreg_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "rfc=RandomForestClassifier()\n",
    "rfc_dict={'max_depth':[2,3,4],\n",
    "          'max_leaf_nodes':[2,3,6],\n",
    "          'min_samples_leaf':[5,4,3,2],\n",
    "          'min_samples_split':[5,3,4],\n",
    "          'n_estimators':[50]\n",
    "        }\n",
    "gcv=GridSearchCV(rfc,param_grid=new_dict,cv=3)\n",
    "gcv.fit(x_train_Tfidf,y_train)\n",
    "gcv_pred=gcv.predict(x_test_Tfidf)\n",
    "gcv_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "gbc=GradientBoostingClassifier()\n",
    "gbc_dict={'max_depth':[2,3,4],\n",
    "          'max_leaf_nodes':[2,3,6],\n",
    "          'min_samples_leaf':[5,4,3,2],\n",
    "          'min_samples_split':[5,3,4],\n",
    "          'n_estimators':[50]\n",
    "        }\n",
    "gcv=GridSearchCV(gbc,param_grid=gbc_dict,cv=3)\n",
    "gcv.fit(x_train_Tfidf,y_train)\n",
    "gcv_pred=gcv.predict(x_test_Tfidf)\n",
    "gcv_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "abc=AdaBoostClassifier()\n",
    "n_dt=({'base_estimator':[1,2,3],\n",
    "        'n_estimators':[50,100,30], \n",
    "       'learning_rate':[1.0,2,8,5],\n",
    "       'loss':['linear','square','exponential'] })\n",
    "gcv=GridSearchCV(abc,param_grid=n_dt,cv=3)\n",
    "\n",
    "abc.fit(x_train_Tfidf,y_train)\n",
    "abc_pred=abc.predict(x_test_Tfidf)\n",
    "abc_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "sgdc=SGDClassifier()\n",
    "n_dict=({'penalty':['l2'],\n",
    "          'alpha':[0.001,0.01,0.0001,0.1],\n",
    "          'l1_ratio':[0.10,0.56,0.34],\n",
    "          'eta0':[1],\n",
    "          'learning_rate':['adaptive','optimal']})\n",
    "gcv=GridSearchCV(sgdc,param_grid=n_dict,cv=3)\n",
    "gcv.fit(x_train_Tfidf,y_train)\n",
    "decreg_pred=gcv.predict(x_test_Tfidf)\n",
    "decreg_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,\n",
       "       1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "sv=SVC()\n",
    "sv_dict=({'C':[1.0,1.1,1.2],\n",
    "           'kernel':['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "           'degree':[3,2,4], 'gamma':[1,2,3] })\n",
    "gcv=GridSearchCV(sv,param_grid=sv_dict,cv=3)\n",
    "gcv.fit(x_train_Tfidf,y_train)\n",
    "\n",
    "sv_pred=gcv.predict(x_test_Tfidf)\n",
    "sv_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
